from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from model import IntentModel
from responses import responses
from llama_cpp import Llama

app = FastAPI()

# Initialize the Llama model only once
class LlamaInference:
    def __init__(self, model_path):
        """
        Initializes the Llama model for inference from a local file.
        :param model_path: The path to the model file.
        """
        self.llm = Llama(model_path=model_path)

    def generate_response(self, instruction, input_text, max_tokens=128, temperature=0.1):
        """
        Generates a response using the Llama model.
        :param instruction: Instruction text for the model.
        :param input_text: Input text for the task.
        :param max_tokens: Maximum tokens for the output.
        :param temperature: Controls the randomness of the generated text (0.0 - 1.0).
        :return: Response text generated by the model.
        """
        alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. 
Write a response that directly completes the request in no more than 3 concise lines. Avoid any greetings, introductions, or additional explanations. Keep it short and to the point.


### Instruction:
{instruction}

### Input:
{input_text}

### Response:
"""

        formatted_query = alpaca_prompt.format(instruction=instruction.strip(), input_text=input_text.strip())

        response = self.llm(formatted_query, max_tokens=max_tokens, temperature=temperature)
        
        return response['choices'][0]['text'].strip()

        #return response['choices'][0]['text']


# Initialize the models at application startup
model_path = "unsloth.Q4_K_M.gguf"
llama_inference = LlamaInference(model_path)
intent_model = IntentModel()


class GenerateRequest(BaseModel):
    instruction: str
    input_text: str


class DetectRequest(BaseModel):
    query: str


@app.post("/generate_response")
async def generate_response(data: GenerateRequest):
    """
    Endpoint to generate a response from the Llama model.
    """
    try:
        # Generate response using the LlamaInference class
        response_text = llama_inference.generate_response(
            instruction=data.instruction,
            input_text=data.input_text,
        )
        return {"response": response_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/train")
async def train(training_data: dict):
    """
    Endpoint to train the intent detection model.
    Expects a JSON payload with training data in the format:
    {
        "intent_name": ["example phrase 1", "example phrase 2"]
    }
    """
    if not training_data:
        raise HTTPException(status_code=400, detail="No training data provided.")

    try:
        intent_model.train(training_data)
        return {"message": "Model trained successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/detect")
async def detect(data: DetectRequest):
    """
    Endpoint to detect intent from user input.
    """
    user_query = data.query
    if not user_query:
        raise HTTPException(status_code=400, detail="No query provided.")

    try:
        detected_intent = intent_model.detect(user_query)
        response_message = responses.get(detected_intent, "No response available for this intent.")
        return {"intent": detected_intent, "response": response_message}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# Run the application using `uvicorn` for FastAPI
# Example command: uvicorn appx:app --reload
